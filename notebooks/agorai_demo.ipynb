{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgorAI Demo: Democratic Multi-Agent Aggregation\n",
    "\n",
    "Welcome to AgorAI! This notebook demonstrates the key features of the library:\n",
    "\n",
    "1. **Core Aggregation** - 14+ methods from social choice theory\n",
    "2. **Benchmarking** - Scientific evaluation with metrics\n",
    "3. **Visualization** - Publication-quality plots and explanations\n",
    "\n",
    "**Version:** 0.2.0  \n",
    "**Date:** November 21, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install AgorAI if needed and import the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install\n",
    "# !pip install -e ..[research]  # From package root\n",
    "# Or: !pip install agorai[research]  # From PyPI (once published)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src')  # Add package to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core modules\n",
    "from agorai.aggregate import aggregate, list_methods\n",
    "from agorai.benchmarks import evaluate_method, compare_methods, list_benchmarks\n",
    "from agorai.visualization import (\n",
    "    plot_utility_matrix,\n",
    "    plot_aggregation_comparison,\n",
    "    plot_fairness_tradeoffs,\n",
    "    explain_decision,\n",
    "    explain_method\n",
    ")\n",
    "\n",
    "# Standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Display settings\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Core Aggregation\n",
    "\n",
    "Let's start with basic aggregation. We'll create a simple scenario with 3 agents voting on 2 candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define Utilities\n",
    "\n",
    "Each row represents an agent's utilities for the candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define utility matrix\n",
    "# Rows = agents, Columns = candidates\n",
    "utilities = [\n",
    "    [0.8, 0.2],  # Agent 1 strongly prefers candidate 0\n",
    "    [0.3, 0.7],  # Agent 2 strongly prefers candidate 1\n",
    "    [0.5, 0.5],  # Agent 3 is indifferent\n",
    "]\n",
    "\n",
    "print(\"Utility Matrix:\")\n",
    "print(f\"Agent 1: {utilities[0]}\")\n",
    "print(f\"Agent 2: {utilities[1]}\")\n",
    "print(f\"Agent 3: {utilities[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Try Different Aggregation Methods\n",
    "\n",
    "Let's see how different methods handle this split decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available methods\n",
    "methods = list_methods()\n",
    "print(f\"Available methods ({len(methods)}):\")\n",
    "for i, method in enumerate(methods, 1):\n",
    "    print(f\"  {i:2d}. {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a few key methods\n",
    "methods_to_try = [\"majority\", \"atkinson\", \"maximin\", \"nash_bargaining\"]\n",
    "\n",
    "print(\"\\nAggregation Results:\\n\" + \"=\"*60)\n",
    "\n",
    "for method in methods_to_try:\n",
    "    result = aggregate(utilities, method=method)\n",
    "    winner = result['winner']\n",
    "    scores = result['scores']\n",
    "    \n",
    "    print(f\"\\n{method.upper():20s}\")\n",
    "    print(f\"  Winner: Candidate {winner}\")\n",
    "    print(f\"  Scores: {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Different methods choose different winners! This is because they optimize for different objectives:\n",
    "- **Majority**: Count votes (candidate with most votes)\n",
    "- **Atkinson**: Balance fairness and efficiency\n",
    "- **Maximin**: Protect worst-off agent\n",
    "- **Nash Bargaining**: Game-theoretic compromise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Benchmarking\n",
    "\n",
    "Now let's use the benchmarking module to scientifically evaluate these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Available Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List built-in benchmarks\n",
    "benchmarks = list_benchmarks()\n",
    "\n",
    "print(\"Available Benchmarks:\\n\" + \"=\"*60)\n",
    "for bench in benchmarks:\n",
    "    print(f\"\\n{bench['name']}:\")\n",
    "    print(f\"  Description: {bench['description']}\")\n",
    "    print(f\"  Test cases: {bench['num_items']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Evaluate Single Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Atkinson method on simple_voting benchmark\n",
    "results = evaluate_method(\n",
    "    method=\"atkinson\",\n",
    "    benchmark=\"simple_voting\",\n",
    "    metrics=[\"fairness\", \"efficiency\", \"agreement\"],\n",
    "    epsilon=1.0\n",
    ")\n",
    "\n",
    "print(\"Atkinson Method Evaluation (Œµ=1.0)\\n\" + \"=\"*60)\n",
    "print(f\"\\nBenchmark: {results['benchmark']}\")\n",
    "print(f\"Test cases: {results['num_items']}\")\n",
    "print(\"\\nSummary Metrics:\")\n",
    "print(\"\\nFairness:\")\n",
    "for metric, value in results['summary']['fairness'].items():\n",
    "    print(f\"  {metric:25s}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nEfficiency:\")\n",
    "for metric, value in results['summary']['efficiency'].items():\n",
    "    print(f\"  {metric:25s}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nAgreement:\")\n",
    "for metric, value in results['summary']['agreement'].items():\n",
    "    print(f\"  {metric:25s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Compare Multiple Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare three methods\n",
    "comparison = compare_methods(\n",
    "    methods=[\"majority\", \"atkinson\", \"maximin\"],\n",
    "    benchmark=\"simple_voting\",\n",
    "    plot=True,  # Generate plots\n",
    "    save_results=\"comparison_results.json\"\n",
    ")\n",
    "\n",
    "print(\"Method Comparison\\n\" + \"=\"*60)\n",
    "print(\"\\nFairness Rankings (Gini Coefficient - lower is better):\")\n",
    "for rank, method in enumerate(comparison['rankings']['fairness_gini_coefficient'], 1):\n",
    "    print(f\"  {rank}. {method}\")\n",
    "\n",
    "print(\"\\nEfficiency Rankings (Social Welfare - higher is better):\")\n",
    "for rank, method in enumerate(comparison['rankings']['efficiency_social_welfare'], 1):\n",
    "    print(f\"  {rank}. {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for method_result in comparison['methods']:\n",
    "    name = method_result['method']\n",
    "    summary = method_result['summary']\n",
    "    \n",
    "    data.append({\n",
    "        'Method': name,\n",
    "        'Gini': summary['fairness']['gini_coefficient'],\n",
    "        'Atkinson Index': summary['fairness']['atkinson_index'],\n",
    "        'Social Welfare': summary['efficiency']['social_welfare'],\n",
    "        'Consensus': summary['agreement']['consensus_score']\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"\\nDetailed Comparison:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Visualization\n",
    "\n",
    "Let's create publication-quality visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Utility Matrix Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize our original utility matrix\n",
    "plot_utility_matrix(\n",
    "    utilities,\n",
    "    agent_labels=[\"Western Perspective\", \"Eastern Perspective\", \"Global South\"],\n",
    "    candidate_labels=[\"Approve\", \"Reject\"],\n",
    "    save_path=\"utility_heatmap.png\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Heatmap saved to: utility_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Method Comparison Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple methods visually\n",
    "plot_aggregation_comparison(\n",
    "    utilities,\n",
    "    methods=[\"majority\", \"atkinson\", \"maximin\", \"nash_bargaining\"],\n",
    "    highlight_differences=True,\n",
    "    save_path=\"method_comparison.png\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Comparison saved to: method_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Fairness-Efficiency Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fairness-efficiency tradeoffs\n",
    "plot_fairness_tradeoffs(\n",
    "    utilities,\n",
    "    methods=[\"majority\", \"borda\", \"atkinson\", \"maximin\", \"nash_bargaining\"],\n",
    "    x_axis=\"social_welfare\",\n",
    "    y_axis=\"gini_coefficient\",\n",
    "    save_path=\"fairness_efficiency_tradeoff.png\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Tradeoff plot saved to: fairness_efficiency_tradeoff.png\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Bottom-right = Ideal (high welfare, low inequality)\")\n",
    "print(\"  - Top-left = Poor (low welfare, high inequality)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Natural Language Explanations\n",
    "\n",
    "AgorAI can explain decisions in plain language!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Explain a Specific Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get aggregation result\n",
    "result = aggregate(utilities, method=\"atkinson\", epsilon=1.0)\n",
    "\n",
    "# Explain the decision\n",
    "explanation = explain_decision(\n",
    "    utilities,\n",
    "    method=\"atkinson\",\n",
    "    winner=result['winner'],\n",
    "    scores=result['scores'],\n",
    "    epsilon=1.0\n",
    ")\n",
    "\n",
    "print(\"Decision Explanation:\\n\" + \"=\"*60)\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Explain How Methods Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get method guide\n",
    "guide = explain_method(\"maximin\")\n",
    "\n",
    "print(\"Maximin Method Guide:\\n\" + \"=\"*60)\n",
    "print(guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Compare Explanations Across Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain how different methods would decide\n",
    "methods_to_explain = [\"majority\", \"atkinson\", \"maximin\"]\n",
    "\n",
    "for method in methods_to_explain:\n",
    "    result = aggregate(utilities, method=method)\n",
    "    explanation = explain_decision(\n",
    "        utilities, method, result['winner'], result['scores']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"METHOD: {method.upper()}\")\n",
    "    print(\"=\"*60)\n",
    "    print(explanation[:300] + \"...\")  # Show first 300 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Advanced Example - Parameter Sweep\n",
    "\n",
    "Let's explore how the Atkinson method behaves with different inequality aversion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep epsilon parameter\n",
    "epsilons = np.linspace(0.0, 2.0, 21)\n",
    "winners = []\n",
    "gini_values = []\n",
    "welfare_values = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    # Run aggregation\n",
    "    result = aggregate(utilities, method=\"atkinson\", epsilon=epsilon)\n",
    "    winner = result['winner']\n",
    "    \n",
    "    # Evaluate fairness and efficiency\n",
    "    benchmark_data = {\n",
    "        'name': 'temp',\n",
    "        'items': [{'utilities': utilities}]\n",
    "    }\n",
    "    eval_result = evaluate_method(\n",
    "        method=\"atkinson\",\n",
    "        benchmark=benchmark_data,\n",
    "        epsilon=epsilon\n",
    "    )\n",
    "    \n",
    "    winners.append(winner)\n",
    "    gini_values.append(eval_result['summary']['fairness']['gini_coefficient'])\n",
    "    welfare_values.append(eval_result['summary']['efficiency']['social_welfare'])\n",
    "\n",
    "print(\"Parameter sweep complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Winner vs epsilon\n",
    "ax1.plot(epsilons, winners, 'o-', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Epsilon (Œµ)', fontsize=12)\n",
    "ax1.set_ylabel('Winner (Candidate ID)', fontsize=12)\n",
    "ax1.set_title('Winner Selection vs Inequality Aversion', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yticks([0, 1])\n",
    "\n",
    "# Gini vs epsilon\n",
    "ax2.plot(epsilons, gini_values, 'o-', color='orange', linewidth=2, markersize=6)\n",
    "ax2.set_xlabel('Epsilon (Œµ)', fontsize=12)\n",
    "ax2.set_ylabel('Gini Coefficient', fontsize=12)\n",
    "ax2.set_title('Fairness vs Inequality Aversion', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Social welfare vs epsilon\n",
    "ax3.plot(epsilons, welfare_values, 'o-', color='green', linewidth=2, markersize=6)\n",
    "ax3.set_xlabel('Epsilon (Œµ)', fontsize=12)\n",
    "ax3.set_ylabel('Social Welfare', fontsize=12)\n",
    "ax3.set_title('Efficiency vs Inequality Aversion', fontsize=13, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('parameter_sweep.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Parameter sweep visualization saved to: parameter_sweep.png\")\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"  - Low Œµ (utilitarian): Winner = Candidate {winners[0]}\")\n",
    "print(f\"  - High Œµ (egalitarian): Winner = Candidate {winners[-1]}\")\n",
    "print(f\"  - Fairness improves as Œµ increases (Gini decreases)\")\n",
    "print(f\"  - Efficiency may decrease as Œµ increases (welfare changes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Real-World Scenario\n",
    "\n",
    "Let's apply AgorAI to a realistic scenario: **Content Moderation with Cultural Diversity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Setup: Multiple Moderators with Different Cultural Backgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: 5 moderators from different regions vote on 3 content decisions\n",
    "# 0 = Remove, 1 = Flag for review, 2 = Allow\n",
    "\n",
    "moderator_utilities = [\n",
    "    [0.9, 0.5, 0.1],  # Western moderator (cautious)\n",
    "    [0.3, 0.6, 0.8],  # Eastern moderator (lenient)\n",
    "    [0.7, 0.7, 0.3],  # Global South moderator (balanced)\n",
    "    [0.8, 0.4, 0.2],  # European moderator (cautious)\n",
    "    [0.4, 0.8, 0.6],  # Middle Eastern moderator (nuanced)\n",
    "]\n",
    "\n",
    "moderator_labels = [\n",
    "    \"Western\",\n",
    "    \"Eastern\",\n",
    "    \"Global South\",\n",
    "    \"European\",\n",
    "    \"Middle Eastern\"\n",
    "]\n",
    "\n",
    "decision_labels = [\"Remove\", \"Flag\", \"Allow\"]\n",
    "\n",
    "print(\"Content Moderation Scenario\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Moderators: {len(moderator_utilities)}\")\n",
    "print(f\"Decisions: {len(decision_labels)}\")\n",
    "print(\"\\nUtilities:\")\n",
    "for label, utils in zip(moderator_labels, moderator_utilities):\n",
    "    print(f\"  {label:15s}: {utils}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Visualize Moderator Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_utility_matrix(\n",
    "    moderator_utilities,\n",
    "    agent_labels=moderator_labels,\n",
    "    candidate_labels=decision_labels,\n",
    "    save_path=\"content_moderation_utilities.png\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Visualization saved to: content_moderation_utilities.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Compare Aggregation Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different aggregation approaches\n",
    "moderation_methods = [\n",
    "    \"majority\",           # Simple majority vote\n",
    "    \"atkinson\",          # Balance fairness and efficiency\n",
    "    \"maximin\",           # Protect minority perspectives\n",
    "    \"nash_bargaining\",   # Game-theoretic compromise\n",
    "]\n",
    "\n",
    "print(\"Aggregation Results for Content Moderation\\n\" + \"=\"*60)\n",
    "\n",
    "for method in moderation_methods:\n",
    "    result = aggregate(moderator_utilities, method=method)\n",
    "    winner = result['winner']\n",
    "    scores = result['scores']\n",
    "    \n",
    "    print(f\"\\n{method.upper():20s}\")\n",
    "    print(f\"  Decision: {decision_labels[winner]}\")\n",
    "    print(f\"  Scores: {[f'{s:.3f}' for s in scores]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Evaluate Fairness Implications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create benchmark from scenario\n",
    "moderation_benchmark = {\n",
    "    'name': 'content_moderation',\n",
    "    'items': [{'utilities': moderator_utilities}]\n",
    "}\n",
    "\n",
    "# Evaluate each method\n",
    "fairness_comparison = []\n",
    "\n",
    "for method in moderation_methods:\n",
    "    eval_result = evaluate_method(\n",
    "        method=method,\n",
    "        benchmark=moderation_benchmark,\n",
    "        metrics=[\"fairness\", \"efficiency\", \"agreement\"]\n",
    "    )\n",
    "    \n",
    "    fairness_comparison.append({\n",
    "        'Method': method,\n",
    "        'Gini': eval_result['summary']['fairness']['gini_coefficient'],\n",
    "        'Atkinson': eval_result['summary']['fairness']['atkinson_index'],\n",
    "        'Social Welfare': eval_result['summary']['efficiency']['social_welfare'],\n",
    "        'Consensus': eval_result['summary']['agreement']['consensus_score']\n",
    "    })\n",
    "\n",
    "# Display comparison\n",
    "df = pd.DataFrame(fairness_comparison)\n",
    "print(\"\\nFairness Analysis:\\n\" + \"=\"*60)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "best_fairness = df.loc[df['Gini'].idxmin(), 'Method']\n",
    "best_welfare = df.loc[df['Social Welfare'].idxmax(), 'Method']\n",
    "print(f\"  - Most fair (lowest Gini): {best_fairness}\")\n",
    "print(f\"  - Most efficient (highest welfare): {best_welfare}\")\n",
    "print(f\"  - Trade-off: Fairness vs efficiency often conflict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Explain the Recommended Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Atkinson for balanced approach\n",
    "result = aggregate(moderator_utilities, method=\"atkinson\", epsilon=1.0)\n",
    "\n",
    "explanation = explain_decision(\n",
    "    moderator_utilities,\n",
    "    method=\"atkinson\",\n",
    "    winner=result['winner'],\n",
    "    scores=result['scores'],\n",
    "    epsilon=1.0\n",
    ")\n",
    "\n",
    "print(\"Recommended Content Moderation Decision:\\n\" + \"=\"*60)\n",
    "print(f\"\\nüéØ Decision: {decision_labels[result['winner']]}\\n\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Next Steps\n",
    "\n",
    "In this demo, you've learned:\n",
    "\n",
    "1. ‚úÖ **Core Aggregation** - Using 14+ methods from social choice theory\n",
    "2. ‚úÖ **Benchmarking** - Scientific evaluation with fairness/efficiency metrics\n",
    "3. ‚úÖ **Visualization** - Creating publication-quality plots\n",
    "4. ‚úÖ **Explanations** - Understanding decisions in plain language\n",
    "5. ‚úÖ **Real-World Application** - Content moderation with cultural diversity\n",
    "\n",
    "### Where to Go Next\n",
    "\n",
    "**Explore More:**\n",
    "- Try different aggregation methods\n",
    "- Create custom benchmarks\n",
    "- Experiment with parameters (epsilon, weights, thresholds)\n",
    "- Apply to your own use cases\n",
    "\n",
    "**Documentation:**\n",
    "- [Aggregation API](../docs/aggregate.md)\n",
    "- [Benchmarking Guide](../docs/benchmarks.md)\n",
    "- [Visualization Guide](../docs/visualization.md)\n",
    "\n",
    "**Research:**\n",
    "- Read the [Research Strategy Report](../../AgorAI_Research_Strategy_Report.md)\n",
    "- Check [AI Research Report](../../AI_Research_2024_2025_Comprehensive_Report.md)\n",
    "- Explore connections to Constitutional AI, EBMs, MARL\n",
    "\n",
    "---\n",
    "\n",
    "**Built with ‚ù§Ô∏è for the democratic AI research community**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
